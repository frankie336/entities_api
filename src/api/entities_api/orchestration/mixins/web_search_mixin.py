# src/api/entities_api/orchestration/mixins/web_search_mixin.py
from __future__ import annotations

import asyncio
import re
from collections import defaultdict
from typing import Any, AsyncGenerator, Dict, Optional

from projectdavid.events import StatusEvent
from projectdavid_common import ToolValidator
from projectdavid_common.utilities.logging_service import LoggingUtility
from projectdavid_common.validation import StatusEnum

from src.api.entities_api.utils.level3_utils import create_status_payload

LOG = LoggingUtility()

# Hard cap on scroll_web_page calls per URL per research session.
# After this many scrolls on a single URL, the tool returns a blocking
# instruction instead of executing, forcing the worker to pivot.
SCROLL_LIMIT_PER_URL = 3


class WebSearchMixin:
    """
    Drive the **Level 3 Agentic Web Tools** (read, scroll, search, discovery).

    Features:
    - Lazy-loaded session state (crash-proof).
    - Query Tiering (adjusts research depth based on complexity).
    - Navigation Guidance (detects pagination).
    - SERP Quality Analysis.
    - Scroll-limit enforcement (prevents doom-scrolling).
    - Search-first gate (blocks scroll when search_web_page hasn't been run).
    """

    # ------------------------------------------------------------------
    # 1. BULLETPROOF STATE MANAGEMENT (Lazy Load)
    # ------------------------------------------------------------------
    @property
    def _research_sessions(self) -> Dict[str, Dict[str, Any]]:
        """
        Safely retrieves the session store.
        If 'OrchestratorCore' didn't initialize it, we do it ourselves here.
        This prevents 'AttributeError' in complex Mixin hierarchies.
        """
        if not hasattr(self, "_internal_sessions_storage"):
            self._internal_sessions_storage = defaultdict(
                lambda: {
                    "sources_read": 0,
                    "urls_visited": set(),
                    # NEW: tracks how many times scroll_web_page has been
                    # called per URL so we can enforce SCROLL_LIMIT_PER_URL.
                    "url_scroll_counts": defaultdict(int),
                    # NEW: tracks which URLs have had search_web_page run
                    # against them, used by the search-first gate.
                    "urls_searched": set(),
                    "query_tier": None,
                    "search_performed": False,
                    "initial_query": None,
                }
            )
        return self._internal_sessions_storage

    # ------------------------------------------------------------------
    # 2. HELPER METHODS & LOGIC
    # ------------------------------------------------------------------

    def _classify_query_tier(self, query: str) -> int:
        """
        Classifies query complexity to determine required source count.

        Returns:
            1: Simple factual query (2 sources)
            2: Moderate multi-faceted query (3-4 sources)
            3: Complex analytical query (5-6 sources)
        """
        query_lower = query.lower()

        tier_3_keywords = [
            "compare",
            "difference",
            "why",
            "analyze",
            "trend",
            "versus",
            "vs",
            "impact of",
            "how do",
            "what are the effects",
        ]
        if any(keyword in query_lower for keyword in tier_3_keywords):
            return 3

        tier_2_keywords = [
            "what are",
            "list",
            "benefits",
            "advantages",
            "disadvantages",
            "types of",
            "examples of",
            "methods",
            "ways to",
        ]
        if any(keyword in query_lower for keyword in tier_2_keywords):
            return 2

        return 1

    def _get_or_create_session(self, run_id: str, query: Optional[str] = None) -> Dict[str, Any]:
        """Get or initialize research session for this run safely."""
        session = self._research_sessions[run_id]

        if query and session["query_tier"] is None:
            session["query_tier"] = self._classify_query_tier(query)
            session["initial_query"] = query
            LOG.info(f"[{run_id}] Query classified as Tier {session['query_tier']}: {query}")

        return session

    async def _validate_research_completeness(
        self,
        run_id: str,
        session: Optional[Dict[str, Any]] = None,
    ) -> tuple[bool, str]:
        """
        Validates if research meets quality standards.
        Useful for internal checks even if not blocking the orchestrator.
        """
        if session is None:
            if run_id not in self._research_sessions:
                return True, "No active research session"
            session = self._research_sessions[run_id]

        query_tier = session.get("query_tier", 1)
        sources_read = session.get("sources_read", 0)
        search_performed = session.get("search_performed", False)

        min_sources = {1: 2, 2: 3, 3: 5}
        required = min_sources.get(query_tier, 2)

        if not search_performed:
            return True, "No web search performed, skipping validation"

        if sources_read < required:
            shortage = required - sources_read
            return False, (
                f"âš ï¸ RESEARCH INCOMPLETE: Tier {query_tier} query requires {required} sources.\n"
                f"Currently read: {sources_read} source(s).\n"
                f"ACTION REQUIRED: Read {shortage} more source(s) before providing final answer."
            )

        return True, f"Research complete: {sources_read}/{required} sources read."

    def _clear_session(self, run_id: str):
        """Clear research session after completion."""
        if run_id in self._research_sessions:
            del self._research_sessions[run_id]

    # ------------------------------------------------------------------
    # NEW: SCROLL GUARD
    # ------------------------------------------------------------------
    def _check_scroll_allowed(self, run_id: str, target_url: str, page_num: int) -> Optional[str]:
        """
        Enforces two scroll safety rules. Returns a blocking error string
        if the scroll should be intercepted, or None if it is permitted.

        Rule 1 â€” Search-first gate:
            scroll_web_page on page > 0 is blocked if search_web_page has
            never been called on this URL. The worker must run search first.
            Page 0 is always allowed because read_web_page loads page 0
            implicitly; blocking it would prevent legitimate first-look reads.

        Rule 2 â€” Hard scroll limit:
            Once SCROLL_LIMIT_PER_URL scrolls have been issued for a URL,
            further calls are blocked and the worker is instructed to pivot.
        """
        session = self._get_or_create_session(run_id)

        # --- Rule 1: search-first gate (only applies beyond page 0) ---
        if page_num > 0 and target_url not in session["urls_searched"]:
            return (
                f"ðŸ›‘ SCROLL BLOCKED â€” search_web_page not yet run on this URL.\n"
                f"MANDATORY NEXT STEP: Call search_web_page('{target_url}', '<your query>') first.\n"
                f"search_web_page scans ALL pages instantly and is always faster than scrolling.\n"
                f"Only return to scroll_web_page if search confirms the relevant section "
                f"and you need surrounding narrative context."
            )

        # --- Rule 2: hard scroll limit ---
        current_count = session["url_scroll_counts"][target_url]
        if current_count >= SCROLL_LIMIT_PER_URL:
            return (
                f"ðŸ›‘ SCROLL LIMIT REACHED â€” scroll_web_page called {current_count} times "
                f"on this URL (limit: {SCROLL_LIMIT_PER_URL}).\n"
                f"MANDATORY ACTION: Stop scrolling '{target_url}'.\n"
                f"You are doom-scrolling. This URL is exhausted. You MUST:\n"
                f"  1. If search_web_page has not been run: call it now with your target query.\n"
                f"  2. If search_web_page already returned nothing: append âš ï¸ to the Scratchpad "
                f"and call perform_web_search to find a different source.\n"
                f"DO NOT call scroll_web_page on this URL again."
            )

        return None  # scroll is permitted

    @staticmethod
    def _format_web_tool_error(tool_name: str, error_content: str, inputs: Dict) -> str:
        """Translates failures into actionable instructions."""
        if not error_content:
            error_content = "Unknown Error (Empty Response)"

        error_lower = error_content.lower()

        if "validation error" in error_lower or "missing arguments" in error_lower:
            return (
                f"âŒ SCHEMA ERROR: Invalid arguments for '{tool_name}'.\n"
                f"ERROR DETAILS: {error_content}\n"
                "SYSTEM INSTRUCTION: Check the tool definition and retry with valid arguments."
            )

        if "403" in error_content or "access denied" in error_lower:
            return (
                f"âŒ Web Tool '{tool_name}' Failed: Access Denied (Bot Protection).\n"
                "SYSTEM INSTRUCTION: This URL is blocking bots. STOP trying this specific URL. "
                "Pick a different URL from your search results."
            )

        if "out of bounds" in error_lower or "invalid page" in error_lower:
            return (
                f"âš ï¸ Web Tool '{tool_name}' Note: End of Document Reached.\n"
                "SYSTEM INSTRUCTION: No more pages. Synthesize collected info."
            )

        return (
            f"âŒ Web Tool '{tool_name}' Error: {error_content}\n"
            "SYSTEM INSTRUCTION: Review arguments. If error persists, stop using this tool."
        )

    def _inject_navigation_guidance(self, content: str, url: str) -> str:
        """Injects hints if content looks like a paginated document."""
        if not content:
            return ""

        page_match = re.search(r"Page\s+(\d+)\s*(?:of|/)\s*(\d+)", content, re.IGNORECASE)
        if not page_match:
            return content

        curr, total = int(page_match.group(1)), int(page_match.group(2))
        next_p = curr + 1

        if curr >= total - 1:
            return content + "\n\n--- âœ… END OF DOCUMENT ---"

        return (
            content + f"\n\n--- ðŸ§­ NAVIGATION (Page {curr}/{total}) ---\n"
            f"1. [STOP] if you have the answer.\n"
            f"2. [SEARCH] 'search_web_page' for keywords.\n"
            f"3. [SCROLL] 'scroll_web_page' to page {next_p} "
            f"(scroll budget remaining: "
            # Note: actual count injected at call-site where session is available
            f"see scroll limit enforcement above)."
        )

    def _parse_serp_results(self, raw_markdown: str, query: str, run_id: str) -> str:
        """Enhanced SERP parsing with quality hints and progress tracking."""
        if not raw_markdown:
            return f"Error: No data for '{query}'."

        session = self._get_or_create_session(run_id, query)
        session["search_performed"] = True

        lines = raw_markdown.split("\n")
        results = []
        count = 0

        for line in lines:
            if count >= 10:
                break
            if "](" in line and "duckduckgo" not in line:
                match = re.search(r"\((https?://[^)]+)\)", line)
                if match:
                    title = line.split("](")[0].strip("[")
                    url = match.group(1)

                    quality_hint = ""
                    if any(domain in url for domain in [".gov", ".edu", ".org"]):
                        quality_hint = " [HIGH AUTHORITY]"
                    elif "wikipedia.org" in url:
                        quality_hint = " [ENCYCLOPEDIA]"

                    results.append(f"{count+1}. **{title}**{quality_hint}\n   URL: {url}")
                    count += 1

        if not results:
            return "No results found."

        query_tier = session["query_tier"]
        min_sources = {1: 2, 2: 3, 3: 5}.get(query_tier, 3)

        output = (
            f"ðŸ” SEARCH RESULTS for '{query}' ({count} found):\n"
            f"ðŸ“Š Query Classification: Tier {query_tier} (Requires {min_sources} sources)\n\n"
            + "\n".join(results)
        )

        output += (
            f"\n\nâš ï¸ MANDATORY NEXT ACTION:\n"
            f"You MUST call read_web_page on at least {min_sources} results above.\n"
            f"Current progress: 0/{min_sources} sources read.\n"
            f"SERP snippets alone are NOT sufficient for answering this query.\n"
            f"Recommended: Read top {min(min_sources + 1, count)} URLs to ensure quality."
        )

        return output

    # ------------------------------------------------------------------
    # 3. CORE EXECUTION LOGIC (The Engine)
    # ------------------------------------------------------------------

    async def _execute_web_tool_logic(
        self,
        tool_name: str,
        required_keys: list[str],
        thread_id: str,
        run_id: str,
        assistant_id: str,
        arguments_dict: Dict[str, Any],
        tool_call_id: Optional[str],
        decision: Optional[Dict],
    ) -> AsyncGenerator[StatusEvent, None]:
        """
        Shared core logic for Read, Scroll, Search, and SERP with progress tracking.
        """
        ts_start = asyncio.get_event_loop().time()

        # --- [1] STATUS: VALIDATING ---
        yield create_status_payload(run_id, tool_name, "Validating parameters...")

        # --- VALIDATION ---
        validator = ToolValidator()
        validator.schema_registry = {tool_name: required_keys}
        validation_error = validator.validate_args(tool_name, arguments_dict)

        if validation_error:
            LOG.warning(f"{tool_name} â–¸ Validation Failed: {validation_error}")
            yield create_status_payload(run_id, tool_name, "Validation failed.", state="error")

            error_feedback = self._format_web_tool_error(
                tool_name, f"Validation Error: {validation_error}", arguments_dict
            )

            try:
                action = await asyncio.to_thread(
                    self.project_david_client.actions.create_action,
                    tool_name=tool_name,
                    run_id=run_id,
                    tool_call_id=tool_call_id,
                    function_args=arguments_dict,
                    decision=decision,
                )

                await self.submit_tool_output(
                    thread_id=thread_id,
                    assistant_id=assistant_id,
                    tool_call_id=tool_call_id,
                    content=error_feedback,
                    action=action,
                    is_error=True,
                )

                await asyncio.to_thread(
                    self.project_david_client.actions.update_action,
                    action_id=action.id,
                    status=StatusEnum.failed.value,
                )
            except Exception as e:
                LOG.error(f"Failed to submit validation error: {e}")

            return

        # --- [2] STATUS: CREATING ACTION (Success Path) ---
        yield create_status_payload(run_id, tool_name, "Initializing tool action...")

        try:
            action = await asyncio.to_thread(
                self.project_david_client.actions.create_action,
                tool_name=tool_name,
                run_id=run_id,
                tool_call_id=tool_call_id,
                function_args=arguments_dict,
                decision=decision,
            )
        except Exception as e:
            LOG.error(f"{tool_name} â–¸ Action creation failed: {e}")
            yield create_status_payload(run_id, tool_name, "Internal system error.", state="error")
            return

        # --- [3] EXECUTION (Threaded I/O) ---
        try:
            final_content = ""

            if tool_name == "read_web_page":
                target_url = arguments_dict["url"]
                yield create_status_payload(run_id, tool_name, f"Reading: {target_url}...")

                session = self._get_or_create_session(run_id)
                if target_url not in session["urls_visited"]:
                    session["urls_visited"].add(target_url)
                    session["sources_read"] += 1

                    LOG.info(
                        f"[{run_id}] Progress: {session['sources_read']} sources read "
                        f"(Tier {session.get('query_tier', 'N/A')} query)"
                    )

                raw_content = await asyncio.to_thread(
                    self.project_david_client.tools.web_read,
                    url=target_url,
                    force_refresh=arguments_dict.get("force_refresh", False),
                )

                tier = session.get("query_tier", 1)
                min_sources = {1: 2, 2: 3, 3: 5}.get(tier, 2)
                progress_note = (
                    f"\n\nðŸ“Š RESEARCH PROGRESS: {session['sources_read']}/{min_sources} sources read "
                    f"(Tier {tier} query)\n"
                    f"âš¡ NEXT STEP: Call search_web_page('{target_url}', '<your query>') "
                    f"to extract facts. Do NOT scroll."
                )

                final_content = (
                    self._inject_navigation_guidance(raw_content, target_url) + progress_note
                )

            elif tool_name == "scroll_web_page":
                target_url = arguments_dict["url"]
                page_num = arguments_dict["page"]

                # --- NEW: SCROLL GUARD ---
                # Intercept before any backend call or action creation.
                scroll_block_msg = self._check_scroll_allowed(run_id, target_url, page_num)
                if scroll_block_msg:
                    LOG.warning(
                        f"[{run_id}] scroll_web_page INTERCEPTED for '{target_url}' "
                        f"page={page_num}: scroll guard triggered."
                    )
                    yield create_status_payload(
                        run_id,
                        tool_name,
                        f"Scroll blocked by guard (page {page_num}).",
                        state="warning",
                    )
                    # Submit the blocking message as tool output so the LLM
                    # receives actionable instructions and can correct course.
                    await self.submit_tool_output(
                        thread_id=thread_id,
                        assistant_id=assistant_id,
                        tool_call_id=tool_call_id,
                        content=scroll_block_msg,
                        action=action,
                        is_error=True,
                    )
                    await asyncio.to_thread(
                        self.project_david_client.actions.update_action,
                        action_id=action.id,
                        status=StatusEnum.failed.value,
                    )
                    return

                # Guard passed â€” increment counter and execute.
                session = self._get_or_create_session(run_id)
                session["url_scroll_counts"][target_url] += 1
                scroll_count = session["url_scroll_counts"][target_url]

                LOG.info(
                    f"[{run_id}] scroll_web_page '{target_url}' page={page_num} "
                    f"(scroll {scroll_count}/{SCROLL_LIMIT_PER_URL})"
                )

                yield create_status_payload(
                    run_id,
                    tool_name,
                    f"Scrolling to page {page_num} "
                    f"(scroll {scroll_count}/{SCROLL_LIMIT_PER_URL})...",
                )

                raw_content = await asyncio.to_thread(
                    self.project_david_client.tools.web_scroll,
                    url=target_url,
                    page=page_num,
                )

                remaining = SCROLL_LIMIT_PER_URL - scroll_count
                scroll_budget_note = (
                    f"\n\nðŸ“œ SCROLL BUDGET: {scroll_count}/{SCROLL_LIMIT_PER_URL} scrolls used "
                    f"on this URL ({remaining} remaining).\n"
                    + (
                        f"âš ï¸ BUDGET EXHAUSTED after this scroll. "
                        f"Run search_web_page or get a new URL."
                        if remaining == 0
                        else f"Prefer search_web_page over further scrolling unless reading a narrative."
                    )
                )

                final_content = (
                    self._inject_navigation_guidance(raw_content, target_url) + scroll_budget_note
                )

            elif tool_name == "search_web_page":
                target_url = arguments_dict["url"]
                query_val = arguments_dict["query"]

                # NEW: Record that search_web_page has been run on this URL.
                # This satisfies the search-first gate for future scroll calls.
                session = self._get_or_create_session(run_id)
                session["urls_searched"].add(target_url)

                yield create_status_payload(
                    run_id, tool_name, f"Searching page for '{query_val}'..."
                )
                final_content = await asyncio.to_thread(
                    self.project_david_client.tools.web_search,
                    url=target_url,
                    query=query_val,
                )

            elif tool_name == "perform_web_search":
                query_val = arguments_dict["query"]
                yield create_status_payload(
                    run_id, tool_name, f"Querying search engine: '{query_val}'..."
                )
                query_str = query_val.replace(" ", "+")
                raw_serp = await asyncio.to_thread(
                    self.project_david_client.tools.web_read,
                    url=f"https://html.duckduckgo.com/html/?q={query_str}",
                    force_refresh=True,
                )
                yield create_status_payload(run_id, tool_name, "Parsing search results...")
                final_content = self._parse_serp_results(raw_serp, query_val, run_id)

            else:
                raise ValueError(f"Unknown web tool: {tool_name}")

            # --- [4] RESULT ANALYSIS ---
            if final_content is None:
                final_content = "âŒ Error: Tool execution returned no data."

            is_soft_failure = "âŒ Error" in final_content or "Error:" in str(final_content)[0:50]

            if is_soft_failure:
                yield create_status_payload(
                    run_id, tool_name, "Encountered external error...", state="warning"
                )
                final_content = self._format_web_tool_error(
                    tool_name, final_content, arguments_dict
                )
            else:
                yield create_status_payload(
                    run_id,
                    tool_name,
                    "Content retrieved successfully.",
                    state="success",
                )

            # --- [5] SUBMIT OUTPUT ---
            await self.submit_tool_output(
                thread_id=thread_id,
                assistant_id=assistant_id,
                tool_call_id=tool_call_id,
                content=final_content,
                action=action,
                is_error=is_soft_failure,
            )

            # --- [6] UPDATE STATUS ---
            await asyncio.to_thread(
                self.project_david_client.actions.update_action,
                action_id=action.id,
                status=(
                    StatusEnum.completed.value if not is_soft_failure else StatusEnum.failed.value
                ),
            )

            LOG.info(
                "[%s] %s completed in %.2fs",
                run_id,
                tool_name,
                asyncio.get_event_loop().time() - ts_start,
            )

        except Exception as exc:
            # --- [7] HARD FAILURE HANDLING ---
            LOG.error(f"[{run_id}] {tool_name} HARD FAILURE: {exc}")
            yield create_status_payload(
                run_id, tool_name, f"Critical failure: {str(exc)}", state="error"
            )

            error_hint = self._format_web_tool_error(tool_name, str(exc), arguments_dict)

            try:
                await asyncio.to_thread(
                    self.project_david_client.actions.update_action,
                    action_id=action.id,
                    status=StatusEnum.failed.value,
                )
                await self.submit_tool_output(
                    thread_id=thread_id,
                    assistant_id=assistant_id,
                    tool_call_id=tool_call_id,
                    content=error_hint,
                    action=action,
                    is_error=True,
                )
            except Exception:
                pass

    # ------------------------------------------------------------------
    # 4. PUBLIC HANDLERS
    # ------------------------------------------------------------------
    async def handle_perform_web_search(
        self,
        thread_id: str,
        run_id: str,
        assistant_id: str,
        arguments_dict: Dict[str, Any],
        tool_call_id: Optional[str] = None,
        decision: Optional[Dict] = None,
    ) -> AsyncGenerator[StatusEvent, None]:
        """Handler for 'perform_web_search' (Google/DDG Discovery)."""
        async for status in self._execute_web_tool_logic(
            tool_name="perform_web_search",
            required_keys=["query"],
            thread_id=thread_id,
            run_id=run_id,
            assistant_id=assistant_id,
            arguments_dict=arguments_dict,
            tool_call_id=tool_call_id,
            decision=decision,
        ):
            yield status

    async def handle_read_web_page(
        self,
        thread_id: str,
        run_id: str,
        assistant_id: str,
        arguments_dict: Dict[str, Any],
        tool_call_id: Optional[str] = None,
        decision: Optional[Dict] = None,
    ) -> AsyncGenerator[StatusEvent, None]:
        async for status in self._execute_web_tool_logic(
            tool_name="read_web_page",
            required_keys=["url"],
            thread_id=thread_id,
            run_id=run_id,
            assistant_id=assistant_id,
            arguments_dict=arguments_dict,
            tool_call_id=tool_call_id,
            decision=decision,
        ):
            yield status

    async def handle_scroll_web_page(
        self,
        thread_id: str,
        run_id: str,
        assistant_id: str,
        arguments_dict: Dict[str, Any],
        tool_call_id: Optional[str] = None,
        decision: Optional[Dict] = None,
    ) -> AsyncGenerator[StatusEvent, None]:
        async for status in self._execute_web_tool_logic(
            tool_name="scroll_web_page",
            required_keys=["url", "page"],
            thread_id=thread_id,
            run_id=run_id,
            assistant_id=assistant_id,
            arguments_dict=arguments_dict,
            tool_call_id=tool_call_id,
            decision=decision,
        ):
            yield status

    async def handle_search_web_page(
        self,
        thread_id: str,
        run_id: str,
        assistant_id: str,
        arguments_dict: Dict[str, Any],
        tool_call_id: Optional[str] = None,
        decision: Optional[Dict] = None,
    ) -> AsyncGenerator[StatusEvent, None]:
        async for status in self._execute_web_tool_logic(
            tool_name="search_web_page",
            required_keys=["url", "query"],
            thread_id=thread_id,
            run_id=run_id,
            assistant_id=assistant_id,
            arguments_dict=arguments_dict,
            tool_call_id=tool_call_id,
            decision=decision,
        ):
            yield status
