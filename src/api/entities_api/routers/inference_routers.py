import json
import asyncio
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from entities_api.inference.inference_arbiter import InferenceArbiter
from entities_api.inference.inference_provider_selector import InferenceProviderSelector
from entities_api.schemas import StreamRequest
from entities_api.services.logging_service import LoggingUtility

router = APIRouter()
logging_utility = LoggingUtility()

@router.post(
    "/completions",
    summary="Asynchronous completions streaming endpoint",
    response_description="A stream of JSON-formatted completions chunks"
)
async def completions(request: StreamRequest):
    """
    Asynchronously receives a chat request, and streams out each chunk as soon
    as it is generated by the selected inference provider.

    Expected payload example:
    {
        "provider": "Hyperbolic",
        "model": "meta-llama/Llama-3.3-70B-Instruct-Turbo",
        "thread_id": "thread_12345",
        "message_id": "message_12345",
        "run_id": "run_12345",
        "assistant_id": "default"
    }

    Each chunk is yielded as an SSE event. The final message "[DONE]" signals the end of the stream.
    """
    logging_utility.info("Completions streaming endpoint called with payload: %s", request.dict())

    # Initialize arbiter and provider selector.
    arbiter = InferenceArbiter()
    selector = InferenceProviderSelector(arbiter)

    # 1. Provider Selection.
    try:
        logging_utility.info(
            "Selecting provider with provider=%s and model=%s",
            request.provider.value, request.model
        )
        provider_instance = selector.select_provider(
            provider=request.provider.value,
            model=request.model
        )
        logging_utility.info("Provider selected successfully: %s", provider_instance)
    except ValueError as ve:
        logging_utility.error("Provider selection error: %s", str(ve), exc_info=True)
        raise HTTPException(status_code=400, detail=str(ve))

    # 2. Define a synchronous generator that streams chunks without aggregation.
    def stream_chunks():
        idx = 0  # A counter for debugging purposes.
        logging_utility.info("Starting to stream chunks for thread_id=%s", request.thread_id)

        try:
            for chunk in provider_instance.process_conversation(
                thread_id=request.thread_id,
                message_id=request.message_id,
                run_id=request.run_id,
                assistant_id=request.assistant_id,
                model=request.model,
                stream_reasoning=False
            ):
                idx += 1
                try:
                    logging_utility.debug("Chunk %d received: %s", idx, chunk)

                    # If the chunk is None, skip it.
                    if chunk is None:
                        logging_utility.warning("Chunk %d is None; skipping.", idx)
                        continue

                    # If the chunk is a string, attempt to parse it as JSON.
                    if isinstance(chunk, str):
                        try:
                            chunk_data = json.loads(chunk)
                            logging_utility.debug("Chunk %d parsed as JSON: %s", idx, chunk_data)
                        except Exception as exc:
                            logging_utility.warning(
                                "Chunk %d failed JSON decode: %s; using raw string as content.",
                                idx, str(exc)
                            )
                            chunk_data = {"content": chunk}
                    elif isinstance(chunk, dict):
                        chunk_data = chunk
                    else:
                        logging_utility.warning(
                            "Chunk %d is of unexpected type: %s; skipping.", idx, type(chunk)
                        )
                        continue

                    # If chunk_data is not a dictionary, skip it.
                    if not isinstance(chunk_data, dict):
                        logging_utility.error("Chunk %d is not a dict after processing; skipping.", idx)
                        continue

                    content = chunk_data.get("content", "")
                    if not content:
                        logging_utility.warning(
                            "Chunk %d has no 'content' key or is empty: %s", idx, chunk_data
                        )

                    # Yield the chunk as an SSE event.
                    yield "data: " + json.dumps(chunk_data) + "\n\n"

                except Exception as inner_exception:
                    logging_utility.error(
                        "Error processing chunk %d: %s", idx, str(inner_exception), exc_info=True
                    )
                    continue

        except Exception as e:
            logging_utility.error("Error during streaming: %s", str(e), exc_info=True)
        finally:
            # Always send the [DONE] message to signal the end of the stream.
            logging_utility.info("Finished streaming chunks for thread_id=%s", request.thread_id)
            yield "data: [DONE]\n\n"

    # 3. Return an SSE streaming response that directly streams out each chunk.
    try:
        logging_utility.info("Returning streaming response using StreamingResponse.")
        return StreamingResponse(stream_chunks(), media_type="text/event-stream")
    except Exception as e:
        logging_utility.error("Error creating streaming response: %s", str(e), exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
